{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESG Active RL Portfolio Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from stable_baselines3 import DQN, DDPG\n",
    "\n",
    "from alphaQ.utils import download_ticker_data, train_test_split, plot_episodes, sharpe, save_to_s3, load_from_s3\n",
    "from alphaQ.env import PortfolioEnv\n",
    "from alphaQ.agent.features import FeatureExtractor\n",
    "from alphaQ.agent.callbacks import EvalCallback\n",
    "from alphaQ.agent.utils import AgentStrategy, load_model, display_attributes\n",
    "from alphaQ.eval import evalu8, evaluate_baselines\n",
    "\n",
    "import config\n",
    "import config as cfg\n",
    "from config import MODELS, MODEL_PARAMS\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "rcParams = {\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'figure.figsize': (18, 9),\n",
    "    'legend.fontsize': 13,\n",
    "    'axes.labelsize': 14\n",
    "}\n",
    "plt.rcParams.update(rcParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment config\n",
    "EXP_NAME = \"esg_dqn\"  # Experiment name\n",
    "RANDOM_SEED = 41\n",
    "\n",
    "model = 'dqn'\n",
    "\n",
    "train_episodes = 20  # Num pre-training episodes (20 DDPG, 30 DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ticker and market data, split into train, validation and test sets\n",
    "esg_key = 'tradeGPT/prices/esg.pkl'\n",
    "market_key = 'tradeGPT/prices/market.pkl'  # SPY\n",
    "\n",
    "tickers = [\"MSFT\", \"NVDA\", \"GOOGL\", \"LLY\"]  # MSCI USA ESG Leaders Index (USD)\n",
    "\n",
    "if False:\n",
    "    data = download_ticker_data(tickers, start=config.START, end=config.END, columns=['Open', 'High', 'Low', 'Close']).dropna()\n",
    "    save_to_s3(data, esg_key)\n",
    "    market = download_ticker_data('SPY', start=config.START, end=config.END, columns=['Adj Close']).loc[data.index]\n",
    "    save_to_s3(market, market_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_from_s3(esg_key)\n",
    "market = load_from_s3(market_key)\n",
    "\n",
    "train, val, test = train_test_split(data, train_years=12)\n",
    "market_train, market_val, market_test = train_test_split(market, train_years=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_type = config.ACTION_SPACE[model]\n",
    "\n",
    "config.RENDER_ENV = False\n",
    "config.RENDER_FREQ = 2\n",
    "\n",
    "TICKERS = [\"MSFT\", \"NVDA\", \"GOOGL\", \"LLY\"]  # MSCI USA ESG Leaders Index (USD)\n",
    "# tickers = [\"\"]  # S&P Global Clean Energy Index (argue that this is down recently, we need a way to actively trade these stocks)\n",
    "\n",
    "# Train and val environments\n",
    "env = PortfolioEnv(\n",
    "    tickers=tickers,\n",
    "    prices=train, \n",
    "    market_prices=market,\n",
    "    window_length=config.WINDOW_LENGTH,\n",
    "    trading_cost=config.COMMISSION_RATE,\n",
    "    action_space_type=action_space_type,\n",
    "    render=config.RENDER_ENV,\n",
    ")\n",
    "val_env = PortfolioEnv(\n",
    "    tickers=tickers,\n",
    "    prices=val, \n",
    "    market_prices=market_val,\n",
    "    window_length=config.WINDOW_LENGTH,\n",
    "    trading_cost=config.COMMISSION_RATE,\n",
    "    action_space_type=action_space_type,\n",
    "    render=config.RENDER_ENV,\n",
    "    render_mode='val'\n",
    ")\n",
    "test_env = PortfolioEnv(\n",
    "    tickers=tickers,\n",
    "    prices=test, \n",
    "    market_prices=market_test,\n",
    "    window_length=config.WINDOW_LENGTH,\n",
    "    trading_cost=config.COMMISSION_RATE,\n",
    "    action_space_type=action_space_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up model args\n",
    "model_params = MODEL_PARAMS[model]\n",
    "\n",
    "# Set neural network parameters\n",
    "policy_kwargs = {\n",
    "    'features_extractor_class': FeatureExtractor,\n",
    "    'features_extractor_kwargs': {\n",
    "        'features_dim': 16 * model_params['multiplier'] * 4 + 5,\n",
    "        'multiplier': model_params['multiplier']\n",
    "    },\n",
    "    'net_arch': model_params['net_arch'],\n",
    "    'optimizer_kwargs': {\n",
    "#         'weight_decay': 5e-9,  # uncomment to use ridge regularisation\n",
    "    },\n",
    "}\n",
    "\n",
    "# load model hyperparameters\n",
    "model_kwargs = dict(model_params['hyperparams'])\n",
    "# add exploration params\n",
    "model_kwargs.update(model_params['exploration'])\n",
    "\n",
    "# set up action noise (for DDPG)\n",
    "if 'action_noise' in model_kwargs:\n",
    "    # determine dimension of action space\n",
    "    n_actions = env.action_space.shape[-1]\n",
    "    # extract noise sigma from params\n",
    "    sigma = model_kwargs.pop('noise_sigma')\n",
    "    model_kwargs['action_noise'] = config.ACTION_NOISE[model_kwargs['action_noise']](\n",
    "        mean=np.zeros(n_actions), \n",
    "        sigma= sigma * np.ones(n_actions)\n",
    "    )\n",
    "\n",
    "model_kwargs.get('action_noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping. Is kind of already implemented. I think we just cap at 20 episodes,\n",
    "# then say as a limitation, we could try training extremely long runs. Thing is, training is just hella unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "EPISODE: 1 Steps: 2470\n",
      "Training episode reward: -4.157685432693042\n",
      "Eval num_timesteps=2470, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 954.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 954      |\n",
      "|    mean_reward      | 0.77     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.701    |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 2470     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "EPISODE: 2 Steps: 2470\n",
      "Training episode reward: -4.105493503632006\n",
      "Eval num_timesteps=4940, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 954.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 954      |\n",
      "|    mean_reward      | 0.77     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.603    |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 4940     |\n",
      "----------------------------------\n",
      "EPISODE: 3 Steps: 2470\n",
      "Training episode reward: -4.88101361119748\n",
      "Eval num_timesteps=7410, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 954.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 954      |\n",
      "|    mean_reward      | 0.77     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.504    |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 7410     |\n",
      "----------------------------------\n",
      "EPISODE: 4 Steps: 2470\n",
      "Training episode reward: -4.70232070893954\n",
      "Eval num_timesteps=9880, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 954.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 954      |\n",
      "|    mean_reward      | 0.77     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.405    |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 9880     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | -4.46    |\n",
      "|    exploration rate | 0.405    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 383      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total timesteps  | 9880     |\n",
      "----------------------------------\n",
      "EPISODE: 5 Steps: 2470\n",
      "Training episode reward: -3.2974109955412585\n",
      "Eval num_timesteps=12350, episode_reward=-0.89 +/- 0.00\n",
      "Episode length: 954.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 954      |\n",
      "|    mean_reward      | -0.891   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.306    |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 12350    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 5e-05    |\n",
      "|    loss             | 0.00015  |\n",
      "|    n_updates        | 2349     |\n",
      "----------------------------------\n",
      "EPISODE: 6 Steps: 2470\n",
      "Training episode reward: -3.3375749527939558\n",
      "Eval num_timesteps=14820, episode_reward=1.02 +/- 0.00\n",
      "Episode length: 954.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 954      |\n",
      "|    mean_reward      | 1.02     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.208    |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 14820    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 5e-05    |\n",
      "|    loss             | 9.49e-05 |\n",
      "|    n_updates        | 4819     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "EPISODE: 7 Steps: 2470\n",
      "Training episode reward: -2.3381425961403037\n",
      "Eval num_timesteps=17290, episode_reward=-1.40 +/- 0.00\n",
      "Episode length: 954.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 954      |\n",
      "|    mean_reward      | -1.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.109    |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 17290    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 5e-05    |\n",
      "|    loss             | 6.1e-05  |\n",
      "|    n_updates        | 7289     |\n",
      "----------------------------------\n",
      "EPISODE: 8 Steps: 2470\n",
      "Training episode reward: -1.3961720187215434\n",
      "Eval num_timesteps=19760, episode_reward=0.41 +/- 0.00\n",
      "Episode length: 954.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 954      |\n",
      "|    mean_reward      | 0.413    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 19760    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 5e-05    |\n",
      "|    loss             | 0.000107 |\n",
      "|    n_updates        | 9759     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | -3.53    |\n",
      "|    exploration rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 44       |\n",
      "|    time_elapsed     | 448      |\n",
      "|    total timesteps  | 19760    |\n",
      "----------------------------------\n",
      "EPISODE: 9 Steps: 2470\n",
      "Training episode reward: -0.6029275640791955\n",
      "Eval num_timesteps=22230, episode_reward=1.63 +/- 0.00\n",
      "Episode length: 954.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 954      |\n",
      "|    mean_reward      | 1.63     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 22230    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 5e-05    |\n",
      "|    loss             | 6.52e-05 |\n",
      "|    n_updates        | 12229    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "EPISODE: 10 Steps: 2470\n",
      "Training episode reward: -0.6477207726977395\n",
      "Eval num_timesteps=24700, episode_reward=-0.51 +/- 0.00\n",
      "Episode length: 954.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 954      |\n",
      "|    mean_reward      | -0.507   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 24700    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 5e-05    |\n",
      "|    loss             | 0.00014  |\n",
      "|    n_updates        | 14699    |\n",
      "----------------------------------\n",
      "EPISODE: 11 Steps: 2470\n",
      "Training episode reward: -1.2777030233805866\n",
      "Eval num_timesteps=27170, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 954.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 954      |\n",
      "|    mean_reward      | -0.0819  |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 27170    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 5e-05    |\n",
      "|    loss             | 0.000249 |\n",
      "|    n_updates        | 17169    |\n",
      "----------------------------------\n",
      "EPISODE: 12 Steps: 2470\n",
      "Training episode reward: -0.8489762256096509\n",
      "Eval num_timesteps=29640, episode_reward=0.22 +/- 0.00\n",
      "Episode length: 954.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 954      |\n",
      "|    mean_reward      | 0.22     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 29640    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 5e-05    |\n",
      "|    loss             | 6.83e-05 |\n",
      "|    n_updates        | 19639    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | -2.63    |\n",
      "|    exploration rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 33       |\n",
      "|    time_elapsed     | 876      |\n",
      "|    total timesteps  | 29640    |\n",
      "----------------------------------\n",
      "EPISODE: 13 Steps: 2470\n",
      "Training episode reward: -0.9346613283885261\n",
      "Eval num_timesteps=32110, episode_reward=0.06 +/- 0.00\n",
      "Episode length: 954.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 954      |\n",
      "|    mean_reward      | 0.0555   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 32110    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 5e-05    |\n",
      "|    loss             | 8.98e-05 |\n",
      "|    n_updates        | 22109    |\n",
      "----------------------------------\n",
      "EPISODE: 14 Steps: 2470\n",
      "Training episode reward: -0.8451573466920114\n",
      "Eval num_timesteps=34580, episode_reward=1.36 +/- 0.00\n",
      "Episode length: 954.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 954      |\n",
      "|    mean_reward      | 1.36     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 34580    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 5e-05    |\n",
      "|    loss             | 4.51e-05 |\n",
      "|    n_updates        | 24579    |\n",
      "----------------------------------\n",
      "EPISODE: 15 Steps: 2470\n",
      "Training episode reward: -0.4176816137712296\n",
      "Eval num_timesteps=37050, episode_reward=0.54 +/- 0.00\n",
      "Episode length: 954.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 954      |\n",
      "|    mean_reward      | 0.54     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 37050    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 5e-05    |\n",
      "|    loss             | 0.000179 |\n",
      "|    n_updates        | 27049    |\n",
      "----------------------------------\n",
      "EPISODE: 16 Steps: 2470\n",
      "Training episode reward: 0.11162023872936166\n",
      "Eval num_timesteps=39520, episode_reward=1.30 +/- 0.00\n",
      "Episode length: 954.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 954      |\n",
      "|    mean_reward      | 1.3      |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 39520    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 5e-05    |\n",
      "|    loss             | 5.62e-05 |\n",
      "|    n_updates        | 29519    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | -2.1     |\n",
      "|    exploration rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 30       |\n",
      "|    time_elapsed     | 1277     |\n",
      "|    total timesteps  | 39520    |\n",
      "----------------------------------\n",
      "EPISODE: 17 Steps: 2470\n",
      "Training episode reward: -0.44365052412335915\n",
      "Eval num_timesteps=41990, episode_reward=0.87 +/- 0.00\n",
      "Episode length: 954.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 954      |\n",
      "|    mean_reward      | 0.866    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 41990    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 5e-05    |\n",
      "|    loss             | 8.11e-05 |\n",
      "|    n_updates        | 31989    |\n",
      "----------------------------------\n",
      "EPISODE: 18 Steps: 2470\n",
      "Training episode reward: -0.7279359062583252\n",
      "Eval num_timesteps=44460, episode_reward=0.93 +/- 0.00\n",
      "Episode length: 954.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 954      |\n",
      "|    mean_reward      | 0.926    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 44460    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 5e-05    |\n",
      "|    loss             | 0.000161 |\n",
      "|    n_updates        | 34459    |\n",
      "----------------------------------\n",
      "EPISODE: 19 Steps: 2470\n",
      "Training episode reward: -1.1422669361154774\n",
      "Eval num_timesteps=46930, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 954.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 954      |\n",
      "|    mean_reward      | -0.0321  |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 46930    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 5e-05    |\n",
      "|    loss             | 6.86e-05 |\n",
      "|    n_updates        | 36929    |\n",
      "----------------------------------\n",
      "EPISODE: 20 Steps: 2470\n",
      "Training episode reward: -0.600754172817904\n",
      "Eval num_timesteps=49400, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 954.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 954      |\n",
      "|    mean_reward      | 0.804    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 49400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 5e-05    |\n",
      "|    loss             | 7.94e-05 |\n",
      "|    n_updates        | 39399    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.47e+03 |\n",
      "|    ep_rew_mean      | -1.83    |\n",
      "|    exploration rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 29       |\n",
      "|    time_elapsed     | 1678     |\n",
      "|    total timesteps  | 49400    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x74c6569b3610>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train agent from scratch\n",
    "best_model_save_path = f'{config.SAVE_PATH}/{EXP_NAME}/best_model'\n",
    "episode_length = env.prices.shape[0] - config.WINDOW_LENGTH\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env=val_env, \n",
    "    n_eval_episodes=1,\n",
    "    eval_freq=episode_length,\n",
    "    log_path=config.LOG_PATH,\n",
    "    best_model_save_path=best_model_save_path,\n",
    "    verbose=config.CALLBACK_VERBOSE_LEVEL,\n",
    "    warn=False,\n",
    ")\n",
    "agent = MODELS[model](\n",
    "    env=env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=config.TRAIN_VERBOSE_LEVEL, \n",
    "    seed=config.RANDOM_SEED,\n",
    "    **model_kwargs,\n",
    ")\n",
    "\n",
    "agent.learn(total_timesteps=train_episodes*episode_length, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves\n",
    "pd.DataFrame(env.record.episodes).plot()\n",
    "pd.DataFrame(val_env.record.episodes).plot()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20acc1d85ed4341021c93850c725098f352660f3df6a3b59dcdfe94746c44c08"
  },
  "kernelspec": {
   "display_name": "Python 3.7.17 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
